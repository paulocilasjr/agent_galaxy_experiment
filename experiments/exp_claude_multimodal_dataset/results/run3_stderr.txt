22:59:34 | INFO | === AutoGluon Training Wrapper Started ===
22:59:34 | INFO | Working directory: /jetstream2/scratch/main/jobs/74577593/working
22:59:34 | INFO | Command line: /jetstream2/scratch/main/jobs/74577593/tool_files/multimodal_learner.py --input_csv_train train_input.csv --input_csv_test test_input.csv --target_column 3 --sample_id_column 1 --images_zip /jetstream2/scratch/main/jobs/74577593/inputs/dataset_aa1a0699-528f-4f0f-ba9a-23399632190b.dat --missing_image_strategy false --backbone_image swin_base_patch4_window7_224.ms_in22k_ft_in1k --backbone_text microsoft/deberta-v3-base --preset best_quality --eval_metric roc_auc --random_seed 42 --time_limit 7200 --deterministic --validation_size 0.2 --split_probabilities 0.85 0.1 0.05 --threshold 0.12 --hyperparameters __oc____dq__env__dq__:__oc____dq__num_gpus__dq__:1__cc__,__dq__data__dq__:__oc____dq__categorical__dq__:__oc____dq__minimum_cat_count__dq__:2__cc____cc__,__dq__optim__dq__:__oc____dq__focal_loss__dq__:__oc____dq__alpha__dq__:0.75,__dq__gamma__dq__:2.0__cc____cc__,__dq__model__dq__:__oc____dq__timm_image__dq__:__oc____dq__checkpoint_name__dq__:__dq__caformer_b36.sail_in22k_ft_in1k_384__dq____cc____cc____cc__ --output_json /jetstream2/scratch/main/jobs/74577593/outputs/dataset_5c75f93b-242d-4231-84ed-e9240e89041d.dat --output_html /jetstream2/scratch/main/jobs/74577593/outputs/dataset_b356d43e-5e4a-42cc-adda-efa516a98c73.dat --output_config /jetstream2/scratch/main/jobs/74577593/outputs/dataset_62273ad6-b9aa-4b22-bc54-4b5c86400e47.dat
22:59:34 | INFO | Parsed args: {'train_dataset': 'train_input.csv', 'test_dataset': 'test_input.csv', 'target_column': '3', 'output_json': '/jetstream2/scratch/main/jobs/74577593/outputs/dataset_5c75f93b-242d-4231-84ed-e9240e89041d.dat', 'output_html': '/jetstream2/scratch/main/jobs/74577593/outputs/dataset_b356d43e-5e4a-42cc-adda-efa516a98c73.dat', 'output_config': '/jetstream2/scratch/main/jobs/74577593/outputs/dataset_62273ad6-b9aa-4b22-bc54-4b5c86400e47.dat', 'images_zip': ['/jetstream2/scratch/main/jobs/74577593/inputs/dataset_aa1a0699-528f-4f0f-ba9a-23399632190b.dat'], 'missing_image_strategy': 'false', 'threshold': 0.12, 'time_limit': 7200, 'deterministic': True, 'random_seed': 42, 'cross_validation': 'false', 'num_folds': 5, 'epochs': None, 'learning_rate': None, 'batch_size': None, 'num_workers': None, 'num_workers_eval': None, 'backbone_image': 'swin_base_patch4_window7_224.ms_in22k_ft_in1k', 'backbone_text': 'microsoft/deberta-v3-base', 'validation_size': 0.2, 'split_probabilities': [0.85, 0.1, 0.05], 'sample_id_column': '1', 'preset': 'best_quality', 'eval_metric': 'roc_auc', 'hyperparameters': '__oc____dq__env__dq__:__oc____dq__num_gpus__dq__:1__cc__,__dq__data__dq__:__oc____dq__categorical__dq__:__oc____dq__minimum_cat_count__dq__:2__cc____cc__,__dq__optim__dq__:__oc____dq__focal_loss__dq__:__oc____dq__alpha__dq__:0.75,__dq__gamma__dq__:2.0__cc____cc__,__dq__model__dq__:__oc____dq__timm_image__dq__:__oc____dq__checkpoint_name__dq__:__dq__caformer_b36.sail_in22k_ft_in1k_384__dq____cc____cc____cc__'}
22:59:34 | INFO | Cache dirs: TORCH_HOME=./torch_cache HF_HOME=./hf_cache HUGGINGFACE_HUB_CACHE=./hf_cache/hub
22:59:34 | INFO | Deterministic mode enabled (seed=42)
22:59:34 | INFO | Train dataset loaded: 533 rows
22:59:34 | INFO | Test dataset loaded: 134 rows
22:59:34 | INFO | Target column '3' not found; using column #3 header 'target' instead.
22:59:34 | INFO | Sample ID column '1' not found; using column #1 header 'patient_id' instead.
22:59:34 | INFO | Extracting 1 image ZIP(s) to /jetstream2/scratch/main/jobs/74577593/tmp/autogluon_images_a1xib48z
23:03:51 | INFO | Extracted dataset_aa1a0699-528f-4f0f-ba9a-23399632190b.dat
23:03:51 | INFO | Inferred image columns: ['CD3_image_path', 'CD8_image_path']
23:04:08 | INFO | Placeholder image created: /jetstream2/scratch/main/jobs/74577593/tmp/ag_placeholder_xdtc9_mp/placeholder_70016c35d2724da593133d12d9ed01cd.png
23:04:09 | INFO | Filled 13 missing images with placeholder
23:04:10 | INFO | Filled 1 missing images with placeholder
23:04:10 | INFO | After cleanup â†’ train: 533, test: 134
23:04:10 | INFO | External test set â†’ created val split (20%)
23:04:10 | INFO | Final split distribution:
split
train    426
val      107
Name: count, dtype: int64
23:04:10 | INFO | Preprocessing complete â€” ready for AutoGluon training!
23:04:10 | INFO | Final split counts:
split
train    426
val      107
Name: count, dtype: int64
23:04:10 | INFO | Applying custom decision threshold 0.1200 for binary evaluation.
23:04:10 | INFO | Using default training num_workers=8 (heuristic).
23:04:10 | INFO | Using default inference num_workers=8 (heuristic).
23:04:10 | WARNING | Could not parse --hyperparameters: [Errno 36] File name too long: '__oc____dq__env__dq__:__oc____dq__num_gpus__dq__:1__cc__,__dq__data__dq__:__oc____dq__categorical__dq__:__oc____dq__minimum_cat_count__dq__:2__cc____cc__,__dq__optim__dq__:__oc____dq__focal_loss__dq__:__oc____dq__alpha__dq__:0.75,__dq__gamma__dq__:2.0__cc____cc__,__dq__model__dq__:__oc____dq__timm_image__dq__:__oc____dq__checkpoint_name__dq__:__dq__caformer_b36.sail_in22k_ft_in1k_384__dq____cc____cc____cc__'. Ignoring.
23:04:10 | INFO | AutoGluon config prepared: fit={'time_limit': 7200, 'seed': 42, 'presets': 'best_quality'}, hyperparameters keys=['env', 'model', 'env.num_workers', 'env.num_workers_inference', 'model.timm_image.checkpoint_name', 'model.hf_text.checkpoint_name']
23:04:10 | INFO | Fitting AutoGluon with 426 train / 107 val rows (internal test rows: 0, external test provided: True)
No path specified. Models will be saved in: "AutogluonModels/ag-20260211_230410"
=================== System Info ===================
AutoGluon Version:  1.4.0
Python Version:     3.10.12
Operating System:   Linux
Platform Machine:   x86_64
Platform Version:   #1 SMP PREEMPT_DYNAMIC Sat Jan 17 14:23:47 EST 2026
CPU Count:          16
Pytorch Version:    2.7.1+cu126
CUDA Version:       12.6
GPU Count:          1
Memory Avail:       56.39 GB / 58.59 GB (96.2%)
Disk Space Avail:   19671.19 GB / 64000.00 GB (30.7%)
===================================================
AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).
	2 unique label values:  [np.int64(0), np.int64(1)]
	If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])

AutoMM starts to create your model. âœ¨âœ¨âœ¨

To track the learning progress, you can open a terminal and launch Tensorboard:
    ```shell
    # Assume you have installed tensorboard
    tensorboard --logdir /jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410
    ```

Seed set to 42
/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
23:04:17 | INFO | Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)
23:04:19 | INFO | [timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
23:04:19 | INFO | Missing keys (head.fc.weight, head.fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
GPU Count: 1
GPU Count to be Used: 1
GPU 0 Name: GRID A100X-20C
GPU 0 Memory: 1.65GB/20.0GB (Used/Total)

Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name              | Type                | Params | Mode  | FLOPs
--------------------------------------------------------------------------
0 | model             | MultimodalFusionMLP | 273 M  | train | 0    
1 | validation_metric | BinaryAUROC         | 0      | train | 0    
2 | loss_func         | CrossEntropyLoss    | 0      | train | 0    
--------------------------------------------------------------------------
273 M     Trainable params
0         Non-trainable params
273 M     Total params
1,095.538 Total estimated model params size (MB)
787       Modules in train mode
0         Modules in eval mode
0         Total Flops
/usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)
Epoch 0, global step 1: 'val_roc_auc' reached 0.49091 (best 0.49091), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=0-step=1.ckpt' as top 3
Epoch 0, global step 4: 'val_roc_auc' reached 0.60107 (best 0.60107), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=0-step=4.ckpt' as top 3
Epoch 1, global step 5: 'val_roc_auc' reached 0.62701 (best 0.62701), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=1-step=5.ckpt' as top 3
Epoch 1, global step 8: 'val_roc_auc' reached 0.55989 (best 0.62701), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=1-step=8.ckpt' as top 3
Epoch 2, global step 9: 'val_roc_auc' reached 0.63877 (best 0.63877), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=2-step=9.ckpt' as top 3
Epoch 2, global step 12: 'val_roc_auc' reached 0.67620 (best 0.67620), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=2-step=12.ckpt' as top 3
Epoch 3, global step 13: 'val_roc_auc' reached 0.67754 (best 0.67754), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=3-step=13.ckpt' as top 3
Epoch 3, global step 16: 'val_roc_auc' was not in top 3
Epoch 4, global step 17: 'val_roc_auc' was not in top 3
Epoch 4, global step 20: 'val_roc_auc' was not in top 3
Epoch 5, global step 21: 'val_roc_auc' was not in top 3
Epoch 5, global step 24: 'val_roc_auc' reached 0.65080 (best 0.67754), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=5-step=24.ckpt' as top 3
Epoch 6, global step 25: 'val_roc_auc' reached 0.66845 (best 0.67754), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=6-step=25.ckpt' as top 3
Epoch 6, global step 28: 'val_roc_auc' reached 0.70000 (best 0.70000), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=6-step=28.ckpt' as top 3
Epoch 7, global step 29: 'val_roc_auc' reached 0.69840 (best 0.70000), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=7-step=29.ckpt' as top 3
Epoch 7, global step 32: 'val_roc_auc' reached 0.69091 (best 0.70000), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=7-step=32.ckpt' as top 3
Epoch 8, global step 33: 'val_roc_auc' was not in top 3
Epoch 8, global step 36: 'val_roc_auc' was not in top 3
Epoch 9, global step 37: 'val_roc_auc' was not in top 3
Epoch 9, global step 40: 'val_roc_auc' reached 0.69519 (best 0.70000), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=9-step=40.ckpt' as top 3
Epoch 10, global step 41: 'val_roc_auc' reached 0.69652 (best 0.70000), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=10-step=41.ckpt' as top 3
Epoch 10, global step 44: 'val_roc_auc' reached 0.70000 (best 0.70000), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=10-step=44.ckpt' as top 3
Epoch 11, global step 45: 'val_roc_auc' reached 0.70160 (best 0.70160), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=11-step=45.ckpt' as top 3
Epoch 11, global step 48: 'val_roc_auc' reached 0.70160 (best 0.70160), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=11-step=48.ckpt' as top 3
Epoch 12, global step 49: 'val_roc_auc' reached 0.70374 (best 0.70374), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=12-step=49.ckpt' as top 3
Epoch 12, global step 52: 'val_roc_auc' reached 0.70321 (best 0.70374), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=12-step=52.ckpt' as top 3
Epoch 13, global step 53: 'val_roc_auc' reached 0.70481 (best 0.70481), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=13-step=53.ckpt' as top 3
Epoch 13, global step 56: 'val_roc_auc' reached 0.70642 (best 0.70642), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=13-step=56.ckpt' as top 3
Epoch 14, global step 57: 'val_roc_auc' reached 0.70749 (best 0.70749), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=14-step=57.ckpt' as top 3
Epoch 14, global step 60: 'val_roc_auc' reached 0.70989 (best 0.70989), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=14-step=60.ckpt' as top 3
Epoch 15, global step 61: 'val_roc_auc' reached 0.70856 (best 0.70989), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=15-step=61.ckpt' as top 3
Epoch 15, global step 64: 'val_roc_auc' reached 0.70882 (best 0.70989), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=15-step=64.ckpt' as top 3
Epoch 16, global step 65: 'val_roc_auc' reached 0.70909 (best 0.70989), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=16-step=65.ckpt' as top 3
Epoch 16, global step 68: 'val_roc_auc' reached 0.70936 (best 0.70989), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=16-step=68.ckpt' as top 3
Epoch 17, global step 69: 'val_roc_auc' was not in top 3
Epoch 17, global step 72: 'val_roc_auc' reached 0.71016 (best 0.71016), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=17-step=72.ckpt' as top 3
Epoch 18, global step 73: 'val_roc_auc' reached 0.70963 (best 0.71016), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=18-step=73.ckpt' as top 3
Epoch 18, global step 76: 'val_roc_auc' was not in top 3
Epoch 19, global step 77: 'val_roc_auc' reached 0.71070 (best 0.71070), saving model to '/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410/epoch=19-step=77.ckpt' as top 3
Epoch 19, global step 80: 'val_roc_auc' was not in top 3
`Trainer.fit` stopped: `max_epochs=20` reached.
Start to fuse 3 checkpoints via the greedy soup algorithm.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
AutoMM has created your model. ðŸŽ‰ðŸŽ‰ðŸŽ‰

To load the model, use the code below:
    ```python
    from autogluon.multimodal import MultiModalPredictor
    predictor = MultiModalPredictor.load("/jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410")
    ```

If you are not satisfied with the model, try to increase the training time, 
adjust the hyperparameters (https://auto.gluon.ai/stable/tutorials/multimodal/advanced_topics/customization.html),
or post issues on GitHub (https://github.com/autogluon/autogluon/issues).


23:55:38 | INFO | AutoGluon training finished. Model path: /jetstream2/scratch/main/jobs/74577593/working/AutogluonModels/ag-20260211_230410
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
23:59:11 | INFO | Here's the model summary:The model achieved score '0.7042781114578247' on the validation metric 'roc_auc'. The total training time is 0:51:27.435025
23:59:11 | INFO | Evaluation complete; splits: ['Train', 'Validation', 'Test']
23:59:11 | INFO | Transparent metrics by split: {'Train': OrderedDict([('Accuracy', 0.852112676056338), ('Precision', 0.7758620689655172), ('Recall_(Sensitivity/TPR)', 0.47368421052631576), ('F1-Score', 0.5882352941176471), ('Specificity_(TNR)', 0.9607250755287009), ('ROC-AUC', 0.8761965336301478), ('PR-AUC', 0.7198763655121129), ('LogLoss', 0.3478437639655766), ('MCC', 0.5272747733713287)]), 'Validation': OrderedDict([('Accuracy', 0.7757009345794392), ('Precision', 0.42857142857142855), ('Recall_(Sensitivity/TPR)', 0.2727272727272727), ('F1-Score', 0.3333333333333333), ('Specificity_(TNR)', 0.9058823529411765), ('ROC-AUC', 0.7106951871657754), ('PR-AUC', 0.39668972213558484), ('LogLoss', 0.5011296964026088), ('MCC', 0.21405243441718844)]), 'Test': OrderedDict([('Accuracy', 0.6194029850746269), ('Precision', 0.36363636363636365), ('Recall_(Sensitivity/TPR)', 0.7272727272727273), ('F1-Score', 0.48484848484848486), ('Specificity_(TNR)', 0.5841584158415841), ('ROC-AUC', 0.7581758175817581), ('PR-AUC', 0.6210813564630533), ('LogLoss', 0.4642165539367009), ('MCC', 0.2683819305394361)])}
23:59:11 | INFO | AutoGluon evaluate() by split: {'Train': {'roc_auc': 0.8761965336301478}, 'Validation': {'roc_auc': 0.7106951871657754}, 'Test': {'roc_auc': 0.7581758175817581}}
23:59:11 | INFO | Wrote full JSON â†’ /jetstream2/scratch/main/jobs/74577593/outputs/dataset_5c75f93b-242d-4231-84ed-e9240e89041d.dat
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
00:01:44 | INFO | Wrote HTML report â†’ /jetstream2/scratch/main/jobs/74577593/outputs/dataset_b356d43e-5e4a-42cc-adda-efa516a98c73.dat
00:01:44 | INFO | Wrote predictor path â†’ predictor_path.txt
00:01:44 | INFO | Wrote AutoGluon config â†’ /jetstream2/scratch/main/jobs/74577593/outputs/dataset_62273ad6-b9aa-4b22-bc54-4b5c86400e47.dat
00:01:44 | INFO | âœ“ Output JSON results: /jetstream2/scratch/main/jobs/74577593/outputs/dataset_5c75f93b-242d-4231-84ed-e9240e89041d.dat (18,358 bytes)
00:01:44 | INFO | âœ“ Output HTML report: /jetstream2/scratch/main/jobs/74577593/outputs/dataset_b356d43e-5e4a-42cc-adda-efa516a98c73.dat (159,252 bytes)
00:01:44 | INFO | âœ“ Output AutoGluon config: /jetstream2/scratch/main/jobs/74577593/outputs/dataset_62273ad6-b9aa-4b22-bc54-4b5c86400e47.dat (4,127 bytes)
