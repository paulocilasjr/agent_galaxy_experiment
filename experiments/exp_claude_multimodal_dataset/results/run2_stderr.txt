22:02:07 | INFO | === AutoGluon Training Wrapper Started ===
22:02:07 | INFO | Working directory: /jetstream2/scratch/main/jobs/74576505/working
22:02:07 | INFO | Command line: /jetstream2/scratch/main/jobs/74576505/tool_files/multimodal_learner.py --input_csv_train train_input.csv --input_csv_test test_input.csv --target_column 3 --sample_id_column 1 --images_zip /jetstream2/scratch/main/jobs/74576505/inputs/dataset_aa1a0699-528f-4f0f-ba9a-23399632190b.dat --missing_image_strategy false --backbone_image swin_base_patch4_window7_224.ms_in22k_ft_in1k --backbone_text microsoft/deberta-v3-base --preset medium_quality --eval_metric roc_auc --random_seed 42 --time_limit 7200 --deterministic --validation_size 0.2 --split_probabilities 0.7 0.1 0.2 --hyperparameters env:__cn__  num_gpus: 1__cn__ --output_json /jetstream2/scratch/main/jobs/74576505/outputs/dataset_e5f3c0b2-2cdd-4665-b43f-7f25f1788803.dat --output_html /jetstream2/scratch/main/jobs/74576505/outputs/dataset_0054b74d-70ee-4677-800e-cf65c4cabc3f.dat --output_config /jetstream2/scratch/main/jobs/74576505/outputs/dataset_740e0031-2e71-4645-9596-7d05a277820f.dat
22:02:07 | INFO | Parsed args: {'train_dataset': 'train_input.csv', 'test_dataset': 'test_input.csv', 'target_column': '3', 'output_json': '/jetstream2/scratch/main/jobs/74576505/outputs/dataset_e5f3c0b2-2cdd-4665-b43f-7f25f1788803.dat', 'output_html': '/jetstream2/scratch/main/jobs/74576505/outputs/dataset_0054b74d-70ee-4677-800e-cf65c4cabc3f.dat', 'output_config': '/jetstream2/scratch/main/jobs/74576505/outputs/dataset_740e0031-2e71-4645-9596-7d05a277820f.dat', 'images_zip': ['/jetstream2/scratch/main/jobs/74576505/inputs/dataset_aa1a0699-528f-4f0f-ba9a-23399632190b.dat'], 'missing_image_strategy': 'false', 'threshold': None, 'time_limit': 7200, 'deterministic': True, 'random_seed': 42, 'cross_validation': 'false', 'num_folds': 5, 'epochs': None, 'learning_rate': None, 'batch_size': None, 'num_workers': None, 'num_workers_eval': None, 'backbone_image': 'swin_base_patch4_window7_224.ms_in22k_ft_in1k', 'backbone_text': 'microsoft/deberta-v3-base', 'validation_size': 0.2, 'split_probabilities': [0.7, 0.1, 0.2], 'sample_id_column': '1', 'preset': 'medium_quality', 'eval_metric': 'roc_auc', 'hyperparameters': 'env:__cn__  num_gpus: 1__cn__'}
22:02:07 | INFO | Cache dirs: TORCH_HOME=./torch_cache HF_HOME=./hf_cache HUGGINGFACE_HUB_CACHE=./hf_cache/hub
22:02:07 | INFO | Deterministic mode enabled (seed=42)
22:02:08 | INFO | Train dataset loaded: 533 rows
22:02:08 | INFO | Test dataset loaded: 134 rows
22:02:08 | INFO | Target column '3' not found; using column #3 header 'target' instead.
22:02:08 | INFO | Sample ID column '1' not found; using column #1 header 'patient_id' instead.
22:02:08 | INFO | Extracting 1 image ZIP(s) to /jetstream2/scratch/main/jobs/74576505/tmp/autogluon_images_mlc9vv61
22:06:58 | INFO | Extracted dataset_aa1a0699-528f-4f0f-ba9a-23399632190b.dat
22:06:58 | INFO | Inferred image columns: ['CD3_image_path', 'CD8_image_path']
22:07:17 | INFO | Placeholder image created: /jetstream2/scratch/main/jobs/74576505/tmp/ag_placeholder_zulme540/placeholder_a9846bce9e764ab4b39ef9ce5c43bc5c.png
22:07:19 | INFO | Filled 13 missing images with placeholder
22:07:20 | INFO | Filled 1 missing images with placeholder
22:07:20 | INFO | After cleanup â†’ train: 533, test: 134
22:07:20 | INFO | External test set â†’ created val split (20%)
22:07:20 | INFO | Final split distribution:
split
train    426
val      107
Name: count, dtype: int64
22:07:20 | INFO | Preprocessing complete â€” ready for AutoGluon training!
22:07:20 | INFO | Final split counts:
split
train    426
val      107
Name: count, dtype: int64
22:07:20 | INFO | Using default training num_workers=8 (heuristic).
22:07:20 | INFO | Using default inference num_workers=8 (heuristic).
22:07:20 | WARNING | Could not parse --hyperparameters: [Errno 2] No such file or directory: 'env:__cn__  num_gpus: 1__cn__'. Ignoring.
22:07:20 | INFO | AutoGluon config prepared: fit={'time_limit': 7200, 'seed': 42, 'presets': 'medium_quality'}, hyperparameters keys=['env', 'model', 'env.num_workers', 'env.num_workers_inference', 'model.timm_image.checkpoint_name', 'model.hf_text.checkpoint_name']
22:07:20 | INFO | Fitting AutoGluon with 426 train / 107 val rows (internal test rows: 0, external test provided: True)
No path specified. Models will be saved in: "AutogluonModels/ag-20260211_220720"
=================== System Info ===================
AutoGluon Version:  1.4.0
Python Version:     3.10.12
Operating System:   Linux
Platform Machine:   x86_64
Platform Version:   #1 SMP PREEMPT_DYNAMIC Sat Jan 17 14:23:47 EST 2026
CPU Count:          16
Pytorch Version:    2.7.1+cu126
CUDA Version:       12.6
GPU Count:          1
Memory Avail:       56.16 GB / 58.59 GB (95.9%)
Disk Space Avail:   19356.07 GB / 64000.00 GB (30.2%)
===================================================
AutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).
	2 unique label values:  [np.int64(0), np.int64(1)]
	If 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])

AutoMM starts to create your model. âœ¨âœ¨âœ¨

To track the learning progress, you can open a terminal and launch Tensorboard:
    ```shell
    # Assume you have installed tensorboard
    tensorboard --logdir /jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720
    ```

Seed set to 42
/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
22:07:34 | INFO | Loading pretrained weights from Hugging Face hub (timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k)
22:07:36 | INFO | [timm/swin_base_patch4_window7_224.ms_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
22:07:36 | INFO | Missing keys (head.fc.weight, head.fc.bias) discovered while loading pretrained weights. This is expected if model is being adapted.
GPU Count: 1
GPU Count to be Used: 1
GPU 0 Name: GRID A100X-20C
GPU 0 Memory: 1.65GB/20.0GB (Used/Total)

Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.

  | Name              | Type                | Params | Mode  | FLOPs
--------------------------------------------------------------------------
0 | model             | MultimodalFusionMLP | 273 M  | train | 0    
1 | validation_metric | BinaryAUROC         | 0      | train | 0    
2 | loss_func         | CrossEntropyLoss    | 0      | train | 0    
--------------------------------------------------------------------------
273 M     Trainable params
0         Non-trainable params
273 M     Total params
1,095.538 Total estimated model params size (MB)
787       Modules in train mode
0         Modules in eval mode
0         Total Flops
Epoch 0, global step 1: 'val_roc_auc' reached 0.49091 (best 0.49091), saving model to '/jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720/epoch=0-step=1.ckpt' as top 3
Epoch 0, global step 4: 'val_roc_auc' reached 0.62166 (best 0.62166), saving model to '/jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720/epoch=0-step=4.ckpt' as top 3
Epoch 1, global step 5: 'val_roc_auc' reached 0.68797 (best 0.68797), saving model to '/jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720/epoch=1-step=5.ckpt' as top 3
Epoch 1, global step 8: 'val_roc_auc' reached 0.73930 (best 0.73930), saving model to '/jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720/epoch=1-step=8.ckpt' as top 3
Epoch 2, global step 9: 'val_roc_auc' reached 0.70267 (best 0.73930), saving model to '/jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720/epoch=2-step=9.ckpt' as top 3
Epoch 2, global step 12: 'val_roc_auc' reached 0.69626 (best 0.73930), saving model to '/jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720/epoch=2-step=12.ckpt' as top 3
Epoch 3, global step 13: 'val_roc_auc' reached 0.74011 (best 0.74011), saving model to '/jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720/epoch=3-step=13.ckpt' as top 3
Epoch 3, global step 16: 'val_roc_auc' reached 0.73850 (best 0.74011), saving model to '/jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720/epoch=3-step=16.ckpt' as top 3
Epoch 4, global step 17: 'val_roc_auc' was not in top 3
Epoch 4, global step 20: 'val_roc_auc' was not in top 3
Epoch 5, global step 21: 'val_roc_auc' was not in top 3
Epoch 5, global step 24: 'val_roc_auc' was not in top 3
Epoch 6, global step 25: 'val_roc_auc' was not in top 3
Epoch 6, global step 28: 'val_roc_auc' was not in top 3
Epoch 7, global step 29: 'val_roc_auc' was not in top 3
Epoch 7, global step 32: 'val_roc_auc' was not in top 3
Epoch 8, global step 33: 'val_roc_auc' was not in top 3
Start to fuse 3 checkpoints via the greedy soup algorithm.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
AutoMM has created your model. ðŸŽ‰ðŸŽ‰ðŸŽ‰

To load the model, use the code below:
    ```python
    from autogluon.multimodal import MultiModalPredictor
    predictor = MultiModalPredictor.load("/jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720")
    ```

If you are not satisfied with the model, try to increase the training time, 
adjust the hyperparameters (https://auto.gluon.ai/stable/tutorials/multimodal/advanced_topics/customization.html),
or post issues on GitHub (https://github.com/autogluon/autogluon/issues).


22:20:44 | INFO | AutoGluon training finished. Model path: /jetstream2/scratch/main/jobs/74576505/working/AutogluonModels/ag-20260211_220720
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
22:25:55 | INFO | Here's the model summary:The model achieved score '0.7082887291908264' on the validation metric 'roc_auc'. The total training time is 0:13:23.897188
22:25:55 | INFO | Evaluation complete; splits: ['Train', 'Validation', 'Test']
22:25:55 | INFO | Transparent metrics by split: {'Train': OrderedDict([('Accuracy', 0.7769953051643192), ('Precision', 0.0), ('Recall_(Sensitivity/TPR)', 0.0), ('F1-Score', 0.0), ('Specificity_(TNR)', 1.0), ('ROC-AUC', 0.7387024964223247), ('PR-AUC', 0.5086485034638352), ('LogLoss', 0.6004903302181227), ('MCC', 0.0)]), 'Validation': OrderedDict([('Accuracy', 0.794392523364486), ('Precision', 0.0), ('Recall_(Sensitivity/TPR)', 0.0), ('F1-Score', 0.0), ('Specificity_(TNR)', 1.0), ('ROC-AUC', 0.7475935828877006), ('PR-AUC', 0.4381420265341867), ('LogLoss', 0.5622092729065217), ('MCC', 0.0)]), 'Test': OrderedDict([('Accuracy', 0.753731343283582), ('Precision', 0.0), ('Recall_(Sensitivity/TPR)', 0.0), ('F1-Score', 0.0), ('Specificity_(TNR)', 1.0), ('ROC-AUC', 0.7139213921392139), ('PR-AUC', 0.6031516443669834), ('LogLoss', 0.6638733564153565), ('MCC', 0.0)])}
22:25:55 | INFO | AutoGluon evaluate() by split: {'Train': {'roc_auc': 0.7387024964223247}, 'Validation': {'roc_auc': 0.7475935828877005}, 'Test': {'roc_auc': 0.7139213921392139}}
22:25:55 | INFO | Wrote full JSON â†’ /jetstream2/scratch/main/jobs/74576505/outputs/dataset_e5f3c0b2-2cdd-4665-b43f-7f25f1788803.dat
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
22:29:22 | INFO | Wrote HTML report â†’ /jetstream2/scratch/main/jobs/74576505/outputs/dataset_0054b74d-70ee-4677-800e-cf65c4cabc3f.dat
22:29:22 | INFO | Wrote predictor path â†’ predictor_path.txt
22:29:22 | INFO | Wrote AutoGluon config â†’ /jetstream2/scratch/main/jobs/74576505/outputs/dataset_740e0031-2e71-4645-9596-7d05a277820f.dat
22:29:22 | INFO | âœ“ Output JSON results: /jetstream2/scratch/main/jobs/74576505/outputs/dataset_e5f3c0b2-2cdd-4665-b43f-7f25f1788803.dat (22,216 bytes)
22:29:22 | INFO | âœ“ Output HTML report: /jetstream2/scratch/main/jobs/74576505/outputs/dataset_0054b74d-70ee-4677-800e-cf65c4cabc3f.dat (156,858 bytes)
22:29:22 | INFO | âœ“ Output AutoGluon config: /jetstream2/scratch/main/jobs/74576505/outputs/dataset_740e0031-2e71-4645-9596-7d05a277820f.dat (4,127 bytes)
